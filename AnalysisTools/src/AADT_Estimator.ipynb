{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AADT Estimator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow detector available count data date range function\n",
    "- find count data date range of a flow detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utility import db_connect, query2csv\n",
    "from settings import  DBNAME, DBPASS, DBUSER, DBHOST\n",
    "\n",
    "def get_date_range(flow_detector_id):\n",
    "    query = \"\"\"\n",
    "select \n",
    "  to_char(min(start_time), 'YYYY-MM-DD HH24:MI:SS') as beginning,\n",
    "  to_char(max(start_time), 'YYYY-MM-DD HH24:MI:SS') as end\n",
    "from \n",
    "  baa_ex_sus.data\n",
    "where \n",
    "  flow_detector_id = {0}\n",
    "\"\"\".format(flow_detector_id)\n",
    "    conn = db_connect()\n",
    "    with conn:\n",
    "        with conn.cursor() as curs:\n",
    "            curs.execute(query)\n",
    "            rows = curs.fetchall()\n",
    "            return (rows[0][0],rows[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2015-07-27 16:00:00', '2017-10-02 00:00:00')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow_detector_id = 1295\n",
    "get_date_range(flow_detector_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get 4 weeks (2016-07-31 - 2016-08-27, within the date range) of count data in 15min bin for flow detector 1295\n",
    " - 4 weeks of raw data in **UTC timezone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Result</h1><a href='./count_data_1295_4weeks.csv?download=1' target='_blank'>count_data_1295_4weeks.csv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utility import db_connect, query2csv\n",
    "from settings import  DBNAME, DBPASS, DBUSER, DBHOST\n",
    "\n",
    "qsql=\"\"\"\n",
    "select \n",
    "  * \n",
    "from \n",
    "  bike_ped.data\n",
    "where \n",
    "  flow_detector_id = 1295\n",
    "  and start_time>='2016-07-31'\n",
    "  and end_time <'2016-08-28'\n",
    "\"\"\"\n",
    "csvfile='count_data_1295_4weeks.csv'\n",
    "query2csv(qsql,csvfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Result</h1><a href='./count_data_1295_localtime_4weeks.csv?download=1' target='_blank'>count_data_1295_localtime_4weeks.csv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utility import db_connect, query2csv\n",
    "from settings import  DBNAME, DBPASS, DBUSER, DBHOST\n",
    "\n",
    "qsql=\"\"\"\n",
    "select \n",
    "  * \n",
    "from \n",
    "  baa_ex_sus.data\n",
    "where \n",
    "  flow_detector_id = 1295\n",
    "  and start_time>='2016-07-31'\n",
    "  and end_time <'2016-08-28'\n",
    "\"\"\"\n",
    "csvfile='count_data_1295_localtime_4weeks.csv'\n",
    "query2csv(qsql,csvfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - get the 4 weeks of raw data and convert the timezone to local time zone and aggregate volume to 1 hour interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Result</h1><a href='./count_data_1295_4weeks_localtz_1hr.csv?download=1' target='_blank'>count_data_1295_4weeks_localtz_1hr.csv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utility import db_connect, query2csv\n",
    "from settings import  DBNAME, DBPASS, DBUSER, DBHOST\n",
    "\n",
    "qsql=\"\"\"\n",
    "with fltz as (\n",
    "  select \n",
    "    flow_detector_id,\n",
    "    bike_ped.get_flow_detector_timezone(flow_detector_id) \n",
    "      as timezone\n",
    "  from \n",
    "    bike_ped.flow_detectors\n",
    "  where\n",
    "    flow_detector_id = 1295  \n",
    "  )\n",
    "  select \n",
    "    bpd.flow_detector_id,\n",
    "    bpd.upload_id,\n",
    "    date_trunc('hour', bpd.start_time) as start_time_utc,\n",
    "    date_trunc('hour', bpd.start_time at time zone fltz.timezone) \n",
    "      as start_time,\n",
    "    fltz.timezone as timezone,\n",
    "    date_trunc('hour', bpd.start_time at time zone fltz.timezone)\n",
    "      +'1 hour'::interval as end_time,\n",
    "    '1 hour'::INTERVAL as measure_period,\n",
    "    sum(volume) as volume\n",
    "from\n",
    "    bike_ped.data as bpd inner join fltz using(flow_detector_id)\n",
    "where \n",
    "    measure_period='00:15:00'\n",
    "    and bpd.flow_detector_id = 1295\n",
    "    and date_trunc('hour', bpd.start_time at time zone fltz.timezone) \n",
    "      >='2016-07-31'\n",
    "    and date_trunc('hour', bpd.start_time at time zone fltz.timezone) \n",
    "      <'2016-08-28'\n",
    "    group by flow_detector_id, upload_id, date_trunc('hour', start_time), \n",
    "      fltz.timezone, \n",
    "      date_trunc('hour', bpd.start_time at time zone fltz.timezone)\n",
    "    having count(start_time) = 4\n",
    "\"\"\"\n",
    "csvfile='count_data_1295_4weeks_localtz_1hr.csv'\n",
    "query2csv(qsql,csvfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - get the 4 weeks of raw data and aggregate to 1 day interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Result</h1><a href='./count_data_1295_4weeks_localtz_1day.csv?download=1' target='_blank'>count_data_1295_4weeks_localtz_1day.csv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utility import db_connect, query2csv\n",
    "from settings import  DBNAME, DBPASS, DBUSER, DBHOST\n",
    "\n",
    "qsql=\"\"\"\n",
    "with fltz as (\n",
    "  select \n",
    "    flow_detector_id,\n",
    "    bike_ped.get_flow_detector_timezone(flow_detector_id) \n",
    "      as timezone\n",
    "  from \n",
    "    bike_ped.flow_detectors\n",
    "  where\n",
    "          flow_detector_id = 1295  \n",
    "  ),\n",
    "hrly as (  \n",
    "  select \n",
    "    bpd.flow_detector_id,\n",
    "    bpd.upload_id,\n",
    "    date_trunc('hour', bpd.start_time) as start_time_utc,\n",
    "    date_trunc('hour', bpd.start_time at time zone fltz.timezone) \n",
    "      as start_time,\n",
    "    fltz.timezone as timezone,\n",
    "    date_trunc('hour', bpd.start_time at time zone fltz.timezone)\n",
    "      +'1 hour'::interval as end_time,\n",
    "    '1 hour'::INTERVAL as measure_period,\n",
    "    sum(volume) as volume\n",
    "from\n",
    "    bike_ped.data as bpd inner join fltz using(flow_detector_id)\n",
    "where \n",
    "    measure_period='00:15:00'\n",
    "    and bpd.flow_detector_id = 1295\n",
    "    and date_trunc('hour', bpd.start_time at time zone fltz.timezone) \n",
    "      >='2016-07-31'\n",
    "    and date_trunc('hour', bpd.start_time at time zone fltz.timezone) \n",
    "      <'2016-08-28'\n",
    "    group by flow_detector_id, upload_id, \n",
    "      date_trunc('hour', start_time), fltz.timezone,\n",
    "      date_trunc('hour', bpd.start_time at time zone fltz.timezone)\n",
    "    having count(start_time) = 4\n",
    ")\n",
    "select\n",
    "    flow_detector_id,\n",
    "    date_trunc('day', start_time) as date,\n",
    "    sum(volume) as count,\n",
    "    extract(dow from start_time) as dow \n",
    "from\n",
    "    hrly\n",
    "group by 1,2,4   \n",
    "order by 2 \n",
    "\"\"\"\n",
    "csvfile='count_data_1295_4weeks_localtz_1day.csv'\n",
    "query2csv(qsql,csvfile)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate WWI from the data\n",
    "<pre>\n",
    "WWI = Vwe/Vwd\n",
    "where:\n",
    "WWI = Weekend/Weekday Index\n",
    "Vwe=average weekend daily traffic\n",
    "Vwd=average weekday daily traffic\n",
    "\n",
    "Weekly grouping by Average WWI metric\n",
    "Weekday Commute: Average WWI <=0.8\n",
    "Weekly Multipurpose: 0.8 < (Average WWI )<=1.2\n",
    "Weekend Multipurpose: Average WWI > 1.2\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utility import db_connect, query2csv\n",
    "from settings import  DBNAME, DBPASS, DBUSER, DBHOST\n",
    "\n",
    "def calculate_wwi(flow_detector_id, start_time, end_time):\n",
    "    query = \"\"\"\n",
    "with fltz as (\n",
    "        select \n",
    "          flow_detector_id,\n",
    "          bike_ped.get_flow_detector_timezone(flow_detector_id) as timezone\n",
    "        from \n",
    "          bike_ped.flow_detectors\n",
    "        where\n",
    "          flow_detector_id = {0}  \n",
    "  ),\n",
    "hrly as (  \n",
    "  select \n",
    "    bpd.flow_detector_id,\n",
    "    bpd.upload_id,\n",
    "    date_trunc('hour', bpd.start_time) as start_time_utc,\n",
    "    date_trunc('hour', bpd.start_time at time zone fltz.timezone) \n",
    "      as start_time,\n",
    "    fltz.timezone as timezone,\n",
    "    date_trunc('hour', bpd.start_time at time zone fltz.timezone)\n",
    "      +'1 hour'::interval as end_time,\n",
    "    '1 hour'::INTERVAL as measure_period,\n",
    "    sum(volume) as volume\n",
    "from\n",
    "    bike_ped.data as bpd inner join fltz using(flow_detector_id)\n",
    "where \n",
    "    measure_period='00:15:00'\n",
    "    and bpd.flow_detector_id = {0}\n",
    "    and date_trunc('hour', bpd.start_time at time zone fltz.timezone) \n",
    "      >='{1}'\n",
    "    and date_trunc('hour', bpd.start_time at time zone fltz.timezone) \n",
    "      <'{2}'\n",
    "    group by flow_detector_id, upload_id, \n",
    "      date_trunc('hour', start_time), \n",
    "      fltz.timezone , \n",
    "      date_trunc('hour', bpd.start_time at time zone fltz.timezone)\n",
    "    having count(start_time) = 4\n",
    "),\n",
    "daily as (\n",
    "select\n",
    "    flow_detector_id,\n",
    "    date_trunc('day', start_time) as date,\n",
    "    sum(volume) as count,\n",
    "    extract(dow from start_time) as dow \n",
    "from\n",
    "    hrly\n",
    "    group by 1,2,4\n",
    "    --Restriction: Do not estimate AADT for days with <22hrs or >26 hours.\n",
    "    having count(*) > 22 and count(*) < 26\n",
    "),\n",
    "-- weekend volume\n",
    "V_we as (\n",
    "  select\n",
    "    avg(count) as vwe\n",
    "  from\n",
    "    daily\n",
    "  where\n",
    "    dow in (0, 6)  \n",
    "),\n",
    "-- weekday volume\n",
    "V_wd as (\n",
    "  select\n",
    "    avg(count) as vwd\n",
    "  from\n",
    "    daily\n",
    "  where\n",
    "    dow in (1,2,3,4,5)  \n",
    ")\n",
    "select round(V_we.vwe/V_wd.vwd, 2) as wwi from V_we, V_wd\n",
    "\"\"\".format(flow_detector_id, start_time, end_time)\n",
    "    conn = db_connect()\n",
    "    with conn:\n",
    "        with conn.cursor() as curs:\n",
    "            curs.execute(query)\n",
    "            rows = curs.fetchall()\n",
    "            return (rows[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The traffic pattern is Weekly Multipurpose with wwi=0.87\n"
     ]
    }
   ],
   "source": [
    "start_time = '2016-07-31'\n",
    "end_time = '2016-08-28'\n",
    "flow_detector_id = 1295\n",
    "wwi = calculate_wwi(flow_detector_id, start_time, end_time)\n",
    "\n",
    "if wwi<=0.8:\n",
    "    wwitype='Weekday Commute'\n",
    "elif wwi>0.8 or wwi <=1.2:\n",
    "    wwitype='Weekly Multipurpose'\n",
    "elif wwi > 1.2:\n",
    "    wwitype='Weekend Multipurpose'\n",
    "print('The traffic pattern is {0} with wwi={1}'.format(wwitype, wwi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the estimated AADT\n",
    "- We are using data for flow detector **1295**\n",
    "- location city is **San Diego**\n",
    "- 4 weeks of data start from **2016-07-31** to **2016-08-27**\n",
    "- mode is **bicycle**\n",
    "- The follow query will return **estimated AADT for each day** for this flow detector in specified date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Result</h1><a href='./est_aadt_flow_detector_1295_4weeks.csv?download=1' target='_blank'>est_aadt_flow_detector_1295_4weeks.csv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utility import db_connect, query2csv\n",
    "from settings import  DBNAME, DBPASS, DBUSER, DBHOST\n",
    "\n",
    "qsql = \"\"\"\n",
    "with d as (\n",
    "  select generate_series(0,6) as dayofweek\n",
    "),\n",
    "m as (\n",
    "  select generate_series(1,12) as month\n",
    "),\n",
    "-- v_ijmy:Compute an average by day of week for each month.\n",
    "v_ijmy as (\n",
    "  select \n",
    "      baadv.analysis_area_id,\n",
    "      date_part('year', baadv.date) as year,\n",
    "      avg(baadv.volume)::bigint as volume_i,\n",
    "      avg(baadv.volume) as volume,\n",
    "      d.dayofweek,\n",
    "      m.month\n",
    "  from\n",
    "      baa_ex_sus.analysis_areas_daily_volume as baadv,\n",
    "      d,\n",
    "      m\n",
    "  where     \n",
    "      extract(dow from baadv.date) in (d.dayofweek)  \n",
    "      AND date_part('month', baadv.date) = m.month\n",
    "      group by baadv.analysis_area_id, year, d.dayofweek, m.month       \n",
    "),\n",
    "-- madt: average volume each month, each year for sites\n",
    "madt as (  \n",
    "  select \n",
    "      analysis_area_id,\n",
    "      month,\n",
    "      year,\n",
    "      avg(volume)::bigint as volume_i,\n",
    "      avg(volume) as volume\n",
    "  from \n",
    "      v_ijmy\n",
    "      group by analysis_area_id, year, month\n",
    "      having count(dayofweek)=7 -- having 7 days of data each week\n",
    "),\n",
    "AADT as (\n",
    "select \n",
    "  analysis_area_id, \n",
    "  year,\n",
    "  avg(volume)::bigint as AADT_i,\n",
    "  round(avg(volume), 2) as AADT\n",
    "from madt\n",
    "  group by analysis_area_id, year\n",
    "  having count(month) = 12 -- having 12 months of data\n",
    "),\n",
    "-- daily_exclude_holiday: daily counts for sites excluding holidays\n",
    "daily_exclude_holiday as (\n",
    "select\n",
    " baaad.analysis_area_id,\n",
    " baaad.date,\n",
    " baaad.volume,\n",
    " date_part('month', baaad.date) as month,\n",
    " date_part('dow', baaad.date) as dow\n",
    "from\n",
    "  baa_ex_sus.analysis_areas_daily_volume as baaad\n",
    "  left join baa.holidays as baahd on baaad.date::date = baahd.holiday_date\n",
    "where\n",
    "  baahd.holiday_id is null\n",
    "  group by 1,2,3\n",
    "),\n",
    "V_jmyl_exclude_holiday as (\n",
    "  select\n",
    "      baadv.analysis_area_id,\n",
    "      date_part('year', baadv.date) as year,\n",
    "      avg(baadv.volume) as volume,\n",
    "      d.dayofweek,\n",
    "      m.month\n",
    "  from\n",
    "      daily_exclude_holiday as baadv,\n",
    "      d,\n",
    "      m\n",
    "  where     \n",
    "      extract(dow from baadv.date) in (d.dayofweek)  \n",
    "      AND date_part('month', baadv.date) = m.month\n",
    "      group by baadv.analysis_area_id, year, d.dayofweek, m.month       \n",
    "),\n",
    "-- 84 factors volume count should exclude holiday weeks\n",
    "factor84 as (\n",
    "select \n",
    "  v_jmyl_nh.analysis_area_id,\n",
    "  v_jmyl_nh.volume as v_jmyl,\n",
    "  AADT.aadt as aadt,\n",
    "  round(v_jmyl_nh.volume/aadt::numeric, 2) as f_jmys,\n",
    "  v_jmyl_nh.dayofweek,\n",
    "  v_jmyl_nh.month,\n",
    "  v_jmyl_nh.year\n",
    "from\n",
    "  V_jmyl_exclude_holiday as v_jmyl_nh inner join AADT \n",
    "    using(analysis_area_id, year)\n",
    "where\n",
    "  AADT.AADT <> 0\n",
    "),\n",
    "V_we as (\n",
    "select \n",
    "  baadv.analysis_area_id,\n",
    "  avg(baadv.volume) vwe\n",
    "from \n",
    "  baa_ex_sus.analysis_areas_daily_volume as baadv\n",
    "where \n",
    "  extract(dow from baadv.date) in (0,6)\n",
    "  group by baadv.analysis_area_id\n",
    "),\n",
    "V_wd as (\n",
    "select \n",
    "  baadv.analysis_area_id,\n",
    "  avg(baadv.volume) vwd\n",
    "from \n",
    "  baa_ex_sus.analysis_areas_daily_volume as baadv\n",
    "where \n",
    "  extract(dow from baadv.date) in (1,2,3,4,5)\n",
    "  group by baadv.analysis_area_id\n",
    "),\n",
    "grouping as (\n",
    "select \n",
    "  V_we.analysis_area_id,\n",
    "  round(V_we.vwe, 2) as V_we,\n",
    "  round(V_wd.vwd, 2) as V_wd,\n",
    "  round(V_we.vwe/V_wd.vwd, 2) as wwi,\n",
    "  case \n",
    "     when (round(V_we.vwe/V_wd.vwd, 2) <= 0.8) \n",
    "       then 'Weekday Commute'\n",
    "     when (round(V_we.vwe/V_wd.vwd, 2) >  1.2) \n",
    "       then 'Weekend Multipurpose'\n",
    "     ELSE 'Weekly Multipurpose'\n",
    "  END as grouping   \n",
    "from \n",
    "  V_we inner join V_wd using (analysis_area_id)\n",
    "),\n",
    "wwi as (\n",
    "select\n",
    "  grouping.analysis_area_id,\n",
    "  baaa.mode,\n",
    "  baaa.analysis_area_name,\n",
    "  baaa.analysis_area_regions_id,\n",
    "  grouping.v_we,\n",
    "  grouping.v_wd,\n",
    "  grouping.wwi,\n",
    "  grouping.grouping as weekly_group\n",
    "from\n",
    "   grouping inner join baa.analysis_areas as baaa \n",
    "     using(analysis_area_id)  \n",
    "),\n",
    "factorgrp as (\n",
    "select \n",
    "  ar.analysis_area_name as city, \n",
    "  wwi.mode,\n",
    "  wwi.weekly_group,\n",
    "  array_agg(wwi.analysis_area_id order by analysis_area_id) \n",
    "    as analysis_area_id_list\n",
    "from \n",
    "  wwi, baa.analysis_area_regions as ar\n",
    "where \n",
    "  ar.analysis_area_regions_id = wwi.analysis_area_regions_id\n",
    "  group by 1,2,3\n",
    "), f84_wwi as (\n",
    "  select\n",
    "    fg.city,\n",
    "    fg.weekly_group,\n",
    "    fg.mode,\n",
    "    fg.analysis_area_id_list,\n",
    "    f84.dayofweek, \n",
    "    f84.month,\n",
    "    f84.year,\n",
    "    round(avg(f84.f_jmys), 2) as f_jmys_avg\n",
    "  from \n",
    "    factor84 as f84 inner join factorgrp as fg\n",
    "    on f84.analysis_area_id = Any(fg.analysis_area_id_list::int[])\n",
    "    group by     \n",
    "    fg.city,\n",
    "    fg.weekly_group,\n",
    "    fg.mode,\n",
    "    fg.analysis_area_id_list,\n",
    "    f84.dayofweek, \n",
    "    f84.month,\n",
    "    f84.year\n",
    "    order by fg.city,\n",
    "    fg.weekly_group,\n",
    "    fg.mode,f84.year, f84.month, f84.dayofweek\n",
    "    ), \n",
    "-- start aadt estimator for a flow detector      \n",
    "fltz as (\n",
    "        select \n",
    "          flow_detector_id,\n",
    "          bike_ped.get_flow_detector_timezone(flow_detector_id) \n",
    "            as timezone\n",
    "        from \n",
    "          bike_ped.flow_detectors\n",
    "        where\n",
    "          flow_detector_id = 1295  \n",
    "  ),\n",
    "hrly as (  \n",
    "  select \n",
    "    bpd.flow_detector_id,\n",
    "    bpd.upload_id,\n",
    "    date_trunc('hour', bpd.start_time) as start_time_utc,\n",
    "    date_trunc('hour', bpd.start_time at time zone fltz.timezone) \n",
    "      as start_time,\n",
    "    fltz.timezone as timezone,\n",
    "    date_trunc('hour', bpd.start_time at time zone fltz.timezone)\n",
    "      +'1 hour'::interval as end_time,\n",
    "    '1 hour'::INTERVAL as measure_period,\n",
    "    sum(volume) as volume\n",
    "from\n",
    "    bike_ped.data as bpd inner join fltz using(flow_detector_id)\n",
    "where \n",
    "    measure_period='00:15:00'\n",
    "    and bpd.flow_detector_id = 1295\n",
    "    and date_trunc('hour', bpd.start_time at time zone fltz.timezone) \n",
    "      >='2016-07-31'\n",
    "    and date_trunc('hour', bpd.start_time at time zone fltz.timezone) \n",
    "      <'2016-08-28'\n",
    "    group by flow_detector_id, upload_id, \n",
    "      date_trunc('hour', start_time), fltz.timezone , \n",
    "      date_trunc('hour', bpd.start_time at time zone fltz.timezone)\n",
    "    having count(start_time) = 4\n",
    "),\n",
    "daily as (\n",
    "select\n",
    "    flow_detector_id,\n",
    "    date_trunc('day', start_time) as date,\n",
    "    sum(volume) as count,\n",
    "    extract(dow from start_time) as dow \n",
    "from\n",
    "    hrly\n",
    "    group by 1,2,4\n",
    "    --Restriction:Do not estimate AADT for days with <22hrs or >26 hours.\n",
    "    having count(*) > 22 and count(*) < 26\n",
    "),\n",
    "-- weekend volume\n",
    "vwe as (\n",
    "  select\n",
    "    avg(count) as vwe\n",
    "  from\n",
    "    daily\n",
    "  where\n",
    "    dow in (0, 6)  \n",
    "),\n",
    "-- weekday volume\n",
    "vwd as (\n",
    "  select\n",
    "    avg(count) as vwd\n",
    "  from\n",
    "    daily\n",
    "  where\n",
    "    dow in (1,2,3,4,5)  \n",
    "),\n",
    "vwwi as (\n",
    "select\n",
    "  daily.flow_detector_id,\n",
    "  daily.date,\n",
    "  date_part('month', daily.date) as month,\n",
    "  date_part('year', daily.date) as year,\n",
    "  daily.count,\n",
    "  daily.dow,\n",
    "  --round(vwe.vwe,2) as vwe,\n",
    "  --round(vwd.vwd,2) as vwd,\n",
    "  round(vwe.vwe/vwd.vwd, 2) as wwi,\n",
    "  case \n",
    "    when (round(vwe.vwe/vwd.vwd, 2) <= 0.8) then 'Weekday Commute'\n",
    "    when (round(vwe.vwe/vwd.vwd, 2) >  1.2) then 'Weekend Multipurpose'\n",
    "    ELSE 'Weekly Multipurpose'\n",
    "  END as grouping\n",
    "  from vwe, vwd, daily\n",
    ")\n",
    "select \n",
    "  vwwi.flow_detector_id,\n",
    "  vwwi.date,\n",
    "  vwwi.month,\n",
    "  vwwi.year,\n",
    "  vwwi.count,\n",
    "  vwwi.grouping,\n",
    "  f84.mode,\n",
    "  round(vwwi.count/f84.f_jmys_avg, 2) as est_aadt\n",
    "from vwwi,f84_wwi as f84\n",
    "where \n",
    "  f84.city = 'San Diego'\n",
    "  and f84.month = vwwi.month\n",
    "  and f84.year = vwwi.year\n",
    "  and f84.weekly_group = vwwi.grouping\n",
    "  and f84.dayofweek = vwwi.dow\n",
    "  and f84.mode = 'bicycle'\n",
    "  order by vwwi.date   \n",
    "\"\"\"\n",
    "csvfile='est_aadt_flow_detector_1295_4weeks.csv'\n",
    "query2csv(qsql,csvfile)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Estimated AADT for sample data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utility import db_connect, query2csv\n",
    "from settings import  DBNAME, DBPASS, DBUSER, DBHOST\n",
    "\n",
    "def calculate_est_aadt(flow_detector_id, start_time, \n",
    "                       end_time, city, mode):\n",
    "    query = \"\"\"\n",
    "with d as (\n",
    "  select generate_series(0,6) as dayofweek\n",
    "),\n",
    "m as (\n",
    "  select generate_series(1,12) as month\n",
    "),\n",
    "-- v_ijmy:Compute an average by day of week for each month.\n",
    "v_ijmy as (\n",
    "  select \n",
    "      baadv.analysis_area_id,\n",
    "      date_part('year', baadv.date) as year,\n",
    "      avg(baadv.volume)::bigint as volume_i,\n",
    "      avg(baadv.volume) as volume,\n",
    "      d.dayofweek,\n",
    "      m.month\n",
    "  from\n",
    "      baa_ex_sus.analysis_areas_daily_volume as baadv,\n",
    "      d,\n",
    "      m\n",
    "  where     \n",
    "      extract(dow from baadv.date) in (d.dayofweek)  \n",
    "      AND date_part('month', baadv.date) = m.month\n",
    "      group by baadv.analysis_area_id, year, d.dayofweek, m.month       \n",
    "),\n",
    "-- madt: average volume each month, each year for sites\n",
    "madt as (  \n",
    "  select \n",
    "      analysis_area_id,\n",
    "      month,\n",
    "      year,\n",
    "      avg(volume)::bigint as volume_i,\n",
    "      avg(volume) as volume\n",
    "  from \n",
    "      v_ijmy\n",
    "      group by analysis_area_id, year, month\n",
    "      having count(dayofweek)=7 -- having 7 days of data each week\n",
    "),\n",
    "AADT as (\n",
    "select \n",
    "  analysis_area_id, \n",
    "  year,\n",
    "  avg(volume)::bigint as AADT_i,\n",
    "  round(avg(volume), 2) as AADT\n",
    "from madt\n",
    "  group by analysis_area_id, year\n",
    "  having count(month) = 12 -- having 12 months of data\n",
    "),\n",
    "-- daily_exclude_holiday: daily counts for sites excluding holidays\n",
    "daily_exclude_holiday as (\n",
    "select\n",
    " baaad.analysis_area_id,\n",
    " baaad.date,\n",
    " baaad.volume,\n",
    " date_part('month', baaad.date) as month,\n",
    " date_part('dow', baaad.date) as dow\n",
    "from\n",
    "  baa_ex_sus.analysis_areas_daily_volume as baaad\n",
    "  left join baa.holidays as baahd \n",
    "    on baaad.date::date = baahd.holiday_date\n",
    "where\n",
    "  baahd.holiday_id is null\n",
    "  group by 1,2,3\n",
    "),\n",
    "V_jmyl_exclude_holiday as (\n",
    "  select\n",
    "      baadv.analysis_area_id,\n",
    "      date_part('year', baadv.date) as year,\n",
    "      avg(baadv.volume) as volume,\n",
    "      d.dayofweek,\n",
    "      m.month\n",
    "  from\n",
    "      daily_exclude_holiday as baadv,\n",
    "      d,\n",
    "      m\n",
    "  where     \n",
    "      extract(dow from baadv.date) in (d.dayofweek)  \n",
    "      AND date_part('month', baadv.date) = m.month\n",
    "      group by baadv.analysis_area_id, year, d.dayofweek, m.month       \n",
    "),\n",
    "-- 84 factors volume count should exclude holiday weeks\n",
    "factor84 as (\n",
    "select \n",
    "  v_jmyl_nh.analysis_area_id,\n",
    "  v_jmyl_nh.volume as v_jmyl,\n",
    "  AADT.aadt as aadt,\n",
    "  round(v_jmyl_nh.volume/aadt::numeric, 2) as f_jmys,\n",
    "  v_jmyl_nh.dayofweek,\n",
    "  v_jmyl_nh.month,\n",
    "  v_jmyl_nh.year\n",
    "from\n",
    "  V_jmyl_exclude_holiday as v_jmyl_nh inner join AADT \n",
    "    using(analysis_area_id, year)\n",
    "where\n",
    "  AADT.AADT <> 0\n",
    "),\n",
    "V_we as (\n",
    "select \n",
    "  baadv.analysis_area_id,\n",
    "  avg(baadv.volume) vwe\n",
    "from \n",
    "  baa_ex_sus.analysis_areas_daily_volume as baadv\n",
    "where \n",
    "  extract(dow from baadv.date) in (0,6)\n",
    "  group by baadv.analysis_area_id\n",
    "),\n",
    "V_wd as (\n",
    "select \n",
    "  baadv.analysis_area_id,\n",
    "  avg(baadv.volume) vwd\n",
    "from \n",
    "  baa_ex_sus.analysis_areas_daily_volume as baadv\n",
    "where \n",
    "  extract(dow from baadv.date) in (1,2,3,4,5)\n",
    "  group by baadv.analysis_area_id\n",
    "),\n",
    "grouping as (\n",
    "select \n",
    "  V_we.analysis_area_id,\n",
    "  round(V_we.vwe, 2) as V_we,\n",
    "  round(V_wd.vwd, 2) as V_wd,\n",
    "  round(V_we.vwe/V_wd.vwd, 2) as wwi,\n",
    "  case \n",
    "     when (round(V_we.vwe/V_wd.vwd, 2) <= 0.8) \n",
    "       then 'Weekday Commute'\n",
    "     when (round(V_we.vwe/V_wd.vwd, 2) >  1.2) \n",
    "       then 'Weekend Multipurpose'\n",
    "     ELSE 'Weekly Multipurpose'\n",
    "  END as grouping   \n",
    "from \n",
    "  V_we inner join V_wd using (analysis_area_id)\n",
    "),\n",
    "wwi as (\n",
    "select\n",
    "  grouping.analysis_area_id,\n",
    "  baaa.mode,\n",
    "  baaa.analysis_area_name,\n",
    "  baaa.analysis_area_regions_id,\n",
    "  grouping.v_we,\n",
    "  grouping.v_wd,\n",
    "  grouping.wwi,\n",
    "  grouping.grouping as weekly_group\n",
    "from\n",
    "   grouping inner join baa.analysis_areas \n",
    "     as baaa using(analysis_area_id)  \n",
    "),\n",
    "factorgrp as (\n",
    "select \n",
    "  ar.analysis_area_name as city, \n",
    "  wwi.mode,\n",
    "  wwi.weekly_group,\n",
    "  array_agg(wwi.analysis_area_id order by analysis_area_id) \n",
    "    as analysis_area_id_list\n",
    "from \n",
    "  wwi, baa.analysis_area_regions as ar\n",
    "where \n",
    "  ar.analysis_area_regions_id = wwi.analysis_area_regions_id\n",
    "  group by 1,2,3\n",
    "), f84_wwi as (\n",
    "  select\n",
    "    fg.city,\n",
    "    fg.weekly_group,\n",
    "    fg.mode,\n",
    "    fg.analysis_area_id_list,\n",
    "    f84.dayofweek, \n",
    "    f84.month,\n",
    "    f84.year,\n",
    "    round(avg(f84.f_jmys), 2) as f_jmys_avg\n",
    "  from \n",
    "    factor84 as f84 inner join factorgrp as fg\n",
    "    on f84.analysis_area_id = Any(fg.analysis_area_id_list::int[])\n",
    "    group by     \n",
    "    fg.city,\n",
    "    fg.weekly_group,\n",
    "    fg.mode,\n",
    "    fg.analysis_area_id_list,\n",
    "    f84.dayofweek, \n",
    "    f84.month,\n",
    "    f84.year\n",
    "    order by fg.city,\n",
    "    fg.weekly_group,\n",
    "    fg.mode,f84.year, f84.month, f84.dayofweek\n",
    "    ), \n",
    "-- start aadt estimator for a flow detector      \n",
    "fltz as (\n",
    "   select \n",
    "     flow_detector_id,\n",
    "     bike_ped.get_flow_detector_timezone(flow_detector_id) \n",
    "       as timezone\n",
    "     from \n",
    "       bike_ped.flow_detectors\n",
    "     where\n",
    "       flow_detector_id = {0}  \n",
    "  ),\n",
    "hrly as (  \n",
    "  select \n",
    "    bpd.flow_detector_id,\n",
    "    bpd.upload_id,\n",
    "    date_trunc('hour', bpd.start_time) as start_time_utc,\n",
    "    date_trunc('hour', bpd.start_time at time zone fltz.timezone) \n",
    "      as start_time,\n",
    "    fltz.timezone as timezone,\n",
    "    date_trunc('hour', bpd.start_time at time zone fltz.timezone)\n",
    "      +'1 hour'::interval as end_time, \n",
    "    '1 hour'::INTERVAL as measure_period,\n",
    "    sum(volume) as volume\n",
    "from\n",
    "    bike_ped.data as bpd inner join fltz using(flow_detector_id)\n",
    "where \n",
    "    measure_period='00:15:00'\n",
    "    and bpd.flow_detector_id = {0}\n",
    "    and date_trunc('hour', bpd.start_time at time zone fltz.timezone) \n",
    "      >='{1}'\n",
    "    and date_trunc('hour', bpd.start_time at time zone fltz.timezone) \n",
    "      <'{2}'\n",
    "    group by flow_detector_id, upload_id, \n",
    "      date_trunc('hour', start_time), fltz.timezone , \n",
    "      date_trunc('hour', bpd.start_time at time zone fltz.timezone)\n",
    "    having count(start_time) = 4\n",
    "),\n",
    "daily as (\n",
    "select\n",
    "    flow_detector_id,\n",
    "    date_trunc('day', start_time) as date,\n",
    "    sum(volume) as count,\n",
    "    extract(dow from start_time) as dow \n",
    "from\n",
    "    hrly\n",
    "    group by 1,2,4\n",
    "    --Restriction:Do not estimate AADT for days with <22hrs or >26 hours.\n",
    "    having count(*) > 22 and count(*) < 26\n",
    "),\n",
    "-- weekend volume\n",
    "vwe as (\n",
    "  select\n",
    "    avg(count) as vwe\n",
    "  from\n",
    "    daily\n",
    "  where\n",
    "    dow in (0, 6)  \n",
    "),\n",
    "-- weekday volume\n",
    "vwd as (\n",
    "  select\n",
    "    avg(count) as vwd\n",
    "  from\n",
    "    daily\n",
    "  where\n",
    "    dow in (1,2,3,4,5)  \n",
    "),\n",
    "vwwi as (\n",
    "select\n",
    "  daily.flow_detector_id,\n",
    "  daily.date,\n",
    "  date_part('month', daily.date) as month,\n",
    "  date_part('year', daily.date) as year,\n",
    "  daily.count,\n",
    "  daily.dow,\n",
    "  --round(vwe.vwe,2) as vwe,\n",
    "  --round(vwd.vwd,2) as vwd,\n",
    "  round(vwe.vwe/vwd.vwd, 2) as wwi,\n",
    "  case \n",
    "    when (round(vwe.vwe/vwd.vwd, 2) <= 0.8) \n",
    "      then 'Weekday Commute'\n",
    "    when (round(vwe.vwe/vwd.vwd, 2) >  1.2) \n",
    "      then 'Weekend Multipurpose'\n",
    "    ELSE 'Weekly Multipurpose'\n",
    "  END as grouping\n",
    "  from vwe, vwd, daily\n",
    "),\n",
    "est_aadt as (\n",
    "select \n",
    "  vwwi.flow_detector_id,\n",
    "  vwwi.date,\n",
    "  vwwi.month,\n",
    "  vwwi.year,\n",
    "  vwwi.count,\n",
    "  vwwi.grouping,\n",
    "  f84.mode,\n",
    "  round(vwwi.count/f84.f_jmys_avg, 2) as est_aadt\n",
    "from vwwi,f84_wwi as f84\n",
    "where \n",
    "  f84.city = '{3}'\n",
    "  and f84.month = vwwi.month\n",
    "  and f84.year = vwwi.year\n",
    "  and f84.weekly_group = vwwi.grouping\n",
    "  and f84.dayofweek = vwwi.dow\n",
    "  and f84.mode = '{4}'\n",
    "  order by vwwi.date   \n",
    "  )\n",
    "  select\n",
    "  round(avg(est_aadt), 2) as estaadt\n",
    "  from\n",
    "    est_aadt\n",
    "\"\"\".format(flow_detector_id, start_time, end_time, city, mode)\n",
    "    conn = db_connect()\n",
    "    with conn:\n",
    "        with conn.cursor() as curs:\n",
    "            curs.execute(query)\n",
    "            rows = curs.fetchall()\n",
    "            return (rows[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated AADt = 230.42\n"
     ]
    }
   ],
   "source": [
    "flow_detector_id=1295\n",
    "start_time = '2016-07-31'\n",
    "end_time = '2016-08-28'\n",
    "mode = 'bicycle'\n",
    "city='San Diego'\n",
    "est_aadt = calculate_est_aadt(flow_detector_id, start_time, \n",
    "                              end_time, city, mode)\n",
    "print('Estimated AADt = {0}'.format(est_aadt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
